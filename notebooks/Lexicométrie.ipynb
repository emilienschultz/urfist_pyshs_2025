{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structurer et explorer des donn√©es textuelles\n",
    "\n",
    "Notebook Introduction au traitement du langage naturel - 15/05/2025 - √âmilien Schultz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Les donn√©es"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Donn√©es questions au gouvernement https://www.data.gouv.fr/fr/datasets/questions-au-gouvernement/\n",
    "\n",
    "T√©l√©charger les donn√©es et les d√©compresser dans un r√©pertoire `data/` dans l'espace de travail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Les biblioth√®ques\n",
    "\n",
    "- `pandas` pour la manipulation de donn√©es\n",
    "- `nltk` pour le traitement de texte\n",
    "- `matplotlib` pour la visualisation\n",
    "- `scikit-learn` pour le traitement de texte et la mod√©lisation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pandas nltk scikit-learn matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structurer les donn√©es\n",
    "\n",
    "Avoir des donn√©es facilement manipulables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fichiers = glob.glob(\"../data/json/*\")\n",
    "fichiers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fichier = json.load(open(fichiers[100], \"r\"))\n",
    "fichier[\"question\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On veut r√©cup√©rer un tableau avec :\n",
    "\n",
    "- la date de la question\n",
    "- la l√©gislature\n",
    "- le texte de la question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numero = fichier[\"question\"][\"identifiant\"][\"numero\"],\n",
    "legislature = fichier[\"question\"][\"identifiant\"][\"legislature\"],\n",
    "date = fichier[\"question\"][\"textesReponse\"][\"texteReponse\"][\"infoJO\"][\"dateJO\"]\n",
    "texte = fichier[\"question\"][\"textesReponse\"][\"texteReponse\"][\"texte\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_info(fichier):\n",
    "    \"\"\"\n",
    "    Fonction qui extrait les informations d'une question parlementaire\n",
    "    \"\"\"\n",
    "    try:\n",
    "        numero = fichier[\"question\"][\"identifiant\"][\"numero\"]\n",
    "        legislature = fichier[\"question\"][\"identifiant\"][\"legislature\"]\n",
    "        date = fichier[\"question\"][\"textesReponse\"][\"texteReponse\"][\"infoJO\"][\"dateJO\"]\n",
    "        texte = fichier[\"question\"][\"textesReponse\"][\"texteReponse\"][\"texte\"]\n",
    "        return {\n",
    "            \"numero\": numero,\n",
    "            \"legislature\": legislature,\n",
    "            \"date\": date,\n",
    "            \"texte\": texte\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors de l'extraction des informations : {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Application au corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = [get_info(json.load(open(fichier, \"r\"))) for fichier in fichiers]\n",
    "t = [i for i in t if i is not None]\n",
    "df = pd.DataFrame(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nettoyer les donn√©es\n",
    "\n",
    "Enlever les balises HTML et les autres soucis. \n",
    "\n",
    "Pour cela on peut utiliser une regex. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remarque sur les regex\n",
    "\n",
    "- biblioth√®que `re` ou `regex`\n",
    "- `re.sub` pour remplacer une partie d'une cha√Æne par une autre\n",
    "- `re.findall` pour trouver toutes les occurrences d'une regex dans une cha√Æne\n",
    "- [Aller voir une cheatsheet ü§ì](https://www.pythoncheatsheet.org/cheatsheet/regular-expressions) ou utiliser un [site d√©di√©](https://regex101.com/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "re.sub(r\"<.*?>\", \"\", \"Ceci est un <b>test</b>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Appliquer au corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Fonction qui nettoie le texte en supprimant les balises HTML et les espaces inutiles\n",
    "    :param text: le texte √† nettoyer\n",
    "    :return: le texte nettoy√©\n",
    "    \"\"\"\n",
    "    text = re.sub(r\"<.*?>\", \"\", text)\n",
    "    text = re.sub(r\"\\s\\s+\", \" \", text)\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Application au corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"texte\"] = df[\"texte\"].apply(clean_text)\n",
    "df[\"texte\"].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"../data/dataframe.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse √† l'√©chelle des mots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chercher la pr√©sence d'un mot\n",
    "\n",
    "Les bases de la fouille de donn√©es. Quels sont les questions qui parlent d'intelligence artificielle ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"texte\"].str.lower().str.contains(\"intelligence artificielle\").sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Et de science ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"texte\"].str.lower().str.contains(\"science|scientifique\").sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Faire une recherche sur toutes les variables possibles de l'IA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenisation\n",
    "\n",
    "D√©couper un texte"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utiliser les regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "word_pattern = r\"\\w+\"\n",
    "tokens = re.findall(word_pattern, \"Ceci est un test\")\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utiliser une premi√®re biblioth√®que : `nltk`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "word_tokenize(\"Ceci est un test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quels sont les termes les plus fr√©quents ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quelles sont les expressions qui reviennent le plus souvent ?\n",
    "\n",
    "Utilisons les bigrammes et les trigrammes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import ngrams\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def generate_bigrams_nltk(text):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    bigrams = list(ngrams(tokens, 2))\n",
    "    return bigrams\n",
    "\n",
    "generate_bigrams_nltk(df[\"texte\"].iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Enlever les stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "french_stopwords = list(set(stopwords.words(\"french\")))\n",
    "french_stopwords[0:10]\n",
    "\n",
    "\n",
    "def generate_bigrams_nltk(text):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    filtered_tokens = [token for token in tokens if token.isalnum() and token not in french_stopwords]\n",
    "    bigrams = list(ngrams(filtered_tokens, 2))\n",
    "    return bigrams\n",
    "\n",
    "generate_bigrams_nltk(df[\"texte\"].iloc[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count bigrams:\n",
    "vectorizer = CountVectorizer(stop_words=french_stopwords, ngram_range=(3, 3), max_features=300)\n",
    "trigrams = (\n",
    "    pd.DataFrame(\n",
    "        vectorizer.fit_transform(df[\"texte\"]).toarray(),\n",
    "        columns=vectorizer.get_feature_names_out(),\n",
    "    )\n",
    "    .T.sum(axis=1)\n",
    "    .sort_values(ascending=False)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Repr√©senter les textes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vecteur brut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words=french_stopwords, ngram_range=(1, 1), max_features=800)\n",
    "X = vectorizer.fit_transform(df[\"texte\"])\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Une version un peu plus avanc√©e\n",
    "\n",
    "- Term Frequency-Inverse Document Frequency\n",
    "    - Am√©lioration du DTM\n",
    "- Approche souvent utilis√©e pour mettre en valeur les mots les plus sp√©cifiques\n",
    "- `Scikit-learn` a `TfidfVectorizer`\n",
    "\n",
    "$$\\text{TF-IDF}(t, d, D) = \\left( \\frac{f_{t,d}}{n_d} \\right) \\times \\log \\left(\\frac{N}{\\text{df}_t} \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words=french_stopwords, ngram_range=(1, 1), max_features=800)\n",
    "X = vectorizer.fit_transform(df[\"texte\"])\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Faire la matrice TF-IDF, identifier les mots qui ont le score le plus important"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distance entre deux textes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "cosine_similarity(X[0], X[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances = pd.DataFrame(pairwise_distances(X, metric=\"cosine\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances[10].idxmax()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application : Faire un nuage de mots avec WordCloud\n",
    "\n",
    "Un coup d'oeil √† la [documentation](https://amueller.github.io/word_cloud/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
