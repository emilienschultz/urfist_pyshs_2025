{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structurer et explorer des donn√©es textuelles\n",
    "\n",
    "Notebook Introduction au traitement du langage naturel - 15/05/2025 - √âmilien Schultz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Les donn√©es"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Donn√©es questions au gouvernement https://www.data.gouv.fr/fr/datasets/questions-au-gouvernement/\n",
    "\n",
    "T√©l√©charger les donn√©es et les d√©compresser dans un r√©pertoire `data/` dans l'espace de travail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Les biblioth√®ques\n",
    "\n",
    "- `pandas` pour la manipulation de donn√©es\n",
    "- `nltk` pour le traitement de texte\n",
    "- `matplotlib` pour la visualisation\n",
    "- `scikit-learn` pour le traitement de texte et la mod√©lisation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pandas nltk scikit-learn matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structurer les donn√©es\n",
    "\n",
    "Avoir des donn√©es facilement manipulables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regarder ce qu'il y a dans un fichier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"question\": {\"@xmlns\": \"http://schemas.assemblee-nationale.fr/referentiel\", \"@xmlns:xsi\": \"http://www.w3.org/2001/XMLSchema-instance\", \"@xsi:type\": \"QuestionGouvernement_Type\", \"uid\": \"QANR5L15QG1001\", \"identifiant\": {\"numero\": \"1001\", \"regime\": \"5eme R\\ufffdpublique\", \"legislature\": \"15\"}, \"type\": \"QG\", \"indexationAN\": {\"rubrique\": \"agriculture\", \"teteAnalyse\": null, \"ANALYSE\": {\"ANA\": \"interdiction du dim\\u00e9thoate\"}}, \"auteur\": {\"identite\": {\"acteurRef\": \"PA721876\", \"mandatRef\": \"PM733719\"}, \"groupe\": {\"organeRef\": \"PO730964\", \"abrege\": \"LAREM\", \"developpe\": \"La R\\u00e9publique en Marche\"}}, \"minInt\": {\"abrege\": \"Agriculture et alimentation\", \"developpe\": \"Minist\\u00e8re de l'agriculture et de l'alimentation\"}, \"minAttribs\": {\"minAttrib\": {\"denomination\": {\"abrege\": \"Agriculture et alimentation\", \"developpe\": \"Minist\\u00e8re de l'agriculture et de l'alimentation\"}}}, \"textesReponse\": {\"texteReponse\": {\"infoJO\": {\"typeJO\": \"JO_DEBAT\", \"dateJO\": \"14/06/2018\"}, \"texte\": \"</p><p align=\\\"CENTER\\\"> INTERDICTION DU DIM\\u00c9THOATE <a name=PG11></a> </p><br><strong>M.\\u00a0le pr\\u00e9sident. </strong>La parole est \\u00e0 M.\\u00a0Adrien Morenas, pour le groupe La R\\u00e9publique en marche.<br><br><strong>M.\\u00a0Adrien Morenas. </strong>Ma question s'adresse \\u00e0 M.\\u00a0St\\u00e9phane Travert, ministre de l'agriculture et de l'alimentation. J'aurais aim\\u00e9 y associer notre coll\\u00e8gue Julien Aubert, absent ce jour, car trop occup\\u00e9 par de futures \\u00e9ch\\u00e9ances \\u00e9lectorales pour se soucier du probl\\u00e8me. <i>(Vives protestations sur les bancs du groupe LR, sur plusieurs bancs du groupe UDI-Agir et parmi les d\\u00e9put\\u00e9s non inscrits.)</i><br><br><strong>M.\\u00a0Thibault Bazin.</strong> Scandaleux\\u00a0!<br><br><strong>M.\\u00a0Maxime Minot.</strong> C'est une honte\\u00a0!<br><br><strong>M.\\u00a0Adrien Morenas. </strong>Monsieur le ministre, le 5\\u00a0mai dernier, un arr\\u00eat\\u00e9 du Gouvernement a autoris\\u00e9 l'importation de cerises en provenance de Turquie. <i>(Protestations continues et claquements de pupitres sur les bancs du groupe LR, sur plusieurs bancs du groupe UDI-Agir et parmi les d\\u00e9put\\u00e9s non inscrits.)</i><br><br><strong>M.\\u00a0Thibault Bazin.</strong> C'est n'importe quoi\\u00a0!<br><br><strong>M.\\u00a0Yves J\\u00e9go.</strong> On n'attaque pas les gens ainsi\\u00a0!<br><br><strong>M.\\u00a0Fabien Di\\u00a0Filippo et M.\\u00a0Christian Jacob .</strong> On ne peut pas laisser passer cela\\u00a0!<br><br><strong>M.\\u00a0le pr\\u00e9sident.</strong> Mes chers coll\\u00e8gues\\u00a0!<br><br><strong>M.\\u00a0Adrien Morenas. </strong>Il y a deux ans, dans un souci de sant\\u00e9 publique, la France interdisait l'utilisation du dim\\u00e9thoate \\u00e0 nos producteurs de cerises, ne leur laissant que quatre produits bien plus chers et surtout bien moins efficaces. En contrepartie, cette d\\u00e9cision fut assortie d'une clause de sauvegarde vis-\\u00e0-vis des pays utilisant le dim\\u00e9thoate. <br><br><strong>M.\\u00a0Marc Le\\u00a0Fur et M.\\u00a0Maurice Leroy .</strong> Lamentable\\u00a0!<br><br><strong>M.\\u00a0Yves J\\u00e9go.</strong> Ce n'est pas l'esprit des questions au Gouvernement\\u00a0!<br><br><strong>M.\\u00a0Philippe Gosselin.</strong> Y a-t-il un pr\\u00e9sident dans la salle\\u00a0? <i>(Les d\\u00e9put\\u00e9s du groupe LR, ainsi que plusieurs d\\u00e9put\\u00e9s du groupe UDI-Agir et plusieurs d\\u00e9put\\u00e9s non inscrits quittent l'h\\u00e9micycle.)</i><br><br><strong>M.\\u00a0le pr\\u00e9sident.</strong> S'il vous pla\\u00eet, mes chers coll\\u00e8gues\\u00a0! Un peu de calme\\u00a0!<br><br><strong>M.\\u00a0Adrien Morenas. </strong>Conscients de ses effets n\\u00e9fastes sur la sant\\u00e9, nos producteurs avaient consenti \\u00e0 modifier leurs habitudes, malgr\\u00e9 l'importante perte de productivit\\u00e9 et les arrachages massifs de cerisiers qui en ont r\\u00e9sult\\u00e9. Le 5\\u00a0mai dernier, en autorisant les importations de cerises turques sur le territoire national, vous avez autoris\\u00e9 l'importation d'un produit ne remplissant pas les conditions de la clause de sauvegarde.<br><br>Je tiens \\u00e0 rappeler ici que la Turquie produit 80\\u00a0000 tonnes de cerises par an, soit vingt fois plus que la France. Mon d\\u00e9partement du Vaucluse fournit 50\\u00a0% de la production nationale. Le signal envoy\\u00e9 \\u00e0 nos producteurs n'est pas bon. Ceux-ci sont en col\\u00e8re et se d\\u00e9couragent face \\u00e0 la lib\\u00e9ralisation des \\u00e9changes qu'ils subissent. D\\u00e8s lors, monsieur le ministre, j'aimerais savoir ce que vous comptez faire afin de prot\\u00e9ger de toute urgence notre \\u00e9conomie agricole et la sant\\u00e9 des consommateurs fran\\u00e7ais.<br><br><strong>M.\\u00a0le pr\\u00e9sident. </strong>Je vous invite, chers coll\\u00e8gues, \\u00e0 poser vos questions sans interpeller un coll\\u00e8gue priv\\u00e9 de la possibilit\\u00e9 de r\\u00e9pondre.<br><br><strong>M.\\u00a0St\\u00e9phane Peu.</strong> Il y a des r\\u00e8gles\\u00a0!<br><br><strong>M.\\u00a0le pr\\u00e9sident. </strong>Je vous invite \\u00e9galement \\u00e0 faire preuve de calme. Tous ici, nous entendons \\u00e0 tour de r\\u00f4le \\u2013\\u00a0moi le premier\\u00a0\\u2013 des propos qui nous \\u00e9corchent les oreilles. Nous devons bien accepter l'expression de la diversit\\u00e9 des opinions dans cette assembl\\u00e9e, qui a pour r\\u00f4le de les repr\\u00e9senter. <i>(Applaudissements sur plusieurs bancs du groupe UDI-Agir.)</i> La parole est \\u00e0 M.\\u00a0le ministre de l'agriculture et de l'alimentation.<br><br><strong>M.\\u00a0St\\u00e9phane Travert,</strong><i> ministre de l'agriculture et de l'alimentation. </i>Monsieur le d\\u00e9put\\u00e9, je vous remercie de votre question, qui me donne l'occasion de r\\u00e9tablir certaines v\\u00e9rit\\u00e9s au sujet des cerises provenant du march\\u00e9 turc.<br><br>Vous le savez, depuis pr\\u00e8s de trois ans, la Turquie a \\u00e9t\\u00e9 vis\\u00e9e par une interdiction d'importation de ses cerises. <i>(M.\\u00a0Julien Aubert entre dans l'h\\u00e9micycle et gagne les bancs du groupe LR.\\u00a0\\u2013 Rires et applaudissements sur plusieurs bancs des groupes UDI-Agir, MODEM et LaREM.)</i><br><br>M.\\u00a0Aubert \\u00e9tant de retour, je peux r\\u00e9pondre \\u00e0 votre question, monsieur Morenas\\u00a0!<br><br>Comme tous les pays, la France a pr\\u00e9vu une clause de sauvegarde en raison des risques aigus encourus par les enfants et mis en \\u00e9vidence par l'Agence nationale de s\\u00e9curit\\u00e9 sanitaire de l\\u2019alimentation, de l\\u2019environnement et du travail.<br><br>Cette ann\\u00e9e, la Turquie a \\u00e9t\\u00e9 autoris\\u00e9e \\u00e0 exporter de nouveau des cerises vers la France, car l'utilisation du dim\\u00e9thoate sur les cerisiers y a \\u00e9t\\u00e9 interdite. Quant \\u00e0 celle des produits phytopharmaceutiques, elle est soumise \\u00e0 une prescription d\\u00e9livr\\u00e9e par un ing\\u00e9nieur r\\u00e9gional agr\\u00e9\\u00e9 par l'\\u00c9tat, et donc fortement encadr\\u00e9e.<br><br>En d\\u00e9but d'ann\\u00e9e, nous avons re\\u00e7u des repr\\u00e9sentants des autorit\\u00e9s turques, lesquelles ont mis en place un plan de contr\\u00f4le permettant de garantir l'absence d'utilisation du dim\\u00e9thoate sur les cerisiers. La Turquie s'est engag\\u00e9e \\u00e0 nous transmettre les r\\u00e9sultats obtenus au terme de la saison de production.<br><br>Ainsi, 196\\u00a0inspecteurs y sont mobilis\\u00e9s. En outre, j'ai demand\\u00e9 que des contr\\u00f4les soient men\\u00e9s sur les cerises arrivant en France. Si un r\\u00e9sultat positif \\u00e9tait av\\u00e9r\\u00e9, l'importation des cerises serait aussit\\u00f4t interdite sur le territoire national.<br><br>\\u00c0 ce jour, 142 visites et douze pr\\u00e9l\\u00e8vements ont \\u00e9t\\u00e9 effectu\\u00e9s ; aucune anomalie n'a \\u00e9t\\u00e9 constat\\u00e9e. Aucune cerise trait\\u00e9e au dim\\u00e9thoate ne franchira la fronti\\u00e8re nationale, je vous l'affirme ! <i>(Applaudissements sur plusieurs bancs du groupe LaREM.)</i><br><br>Par ailleurs, cette position illustre le combat que m\\u00e8ne la France \\u00e0 Bruxelles contre toutes les formes de distorsion de concurrence, ce dont je me f\\u00e9licite. Elle nous a notamment permis d'aboutir cette nuit \\u00e0 un accord avec les jeunes agriculteurs et\\u2026<br><br><strong>M.\\u00a0le pr\\u00e9sident.</strong> Merci, monsieur le ministre. <i>(Applaudissements sur quelques bancs du groupe LaREM.)</i><br> <p>\"}}, \"cloture\": {\"codeCloture\": \"REP_PUB\", \"libelleCloture\": \"R\\u00e9ponse publi\\u00e9e\", \"dateCloture\": \"14/06/2018\", \"infoJO\": {\"typeJO\": \"JO_DEBAT\", \"dateJO\": \"14/06/2018\", \"pageJO\": \"6004\"}}}}\n"
     ]
    }
   ],
   "source": [
    "with open(\"./data/json/QANR5L15QG1001.json\",\"r\") as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Etape de pr√©paration\n",
    "\n",
    "Je veux :\n",
    "- ouvrir chaque fichier\n",
    "- r√©cup√©rer le champ \"text\" de la question\n",
    "- faire un tableau qui contient toutes les donn√©es textuelles pour ensuite faire l'analyse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Faisons-le pour un seul fichier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./data/json/QANR5L15QG3063.json'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fichiers = glob.glob(\"./data/json/*\")\n",
    "fichiers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'15'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fichier = json.load(open(fichiers[100], \"r\"))\n",
    "fichier[\"question\"][\"identifiant\"][\"legislature\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On veut r√©cup√©rer un tableau avec :\n",
    "\n",
    "- la date de la question\n",
    "- ~~la l√©gislature~~\n",
    "- le texte de la question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "numero = fichier[\"question\"][\"identifiant\"][\"numero\"],\n",
    "# legislature = fichier[\"question\"][\"identifiant\"][\"legislature\"],\n",
    "date = fichier[\"question\"][\"textesReponse\"][\"texteReponse\"][\"infoJO\"][\"dateJO\"]\n",
    "texte = fichier[\"question\"][\"textesReponse\"][\"texteReponse\"][\"texte\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('4837',), '23/02/2022')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numero, date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Faire une fonction qui le fait pour n'importe quel fichier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_info(fichier):\n",
    "    \"\"\"\n",
    "    Fonction qui extrait les informations d'une question parlementaire\n",
    "    \"\"\"\n",
    "    try:\n",
    "        numero = fichier[\"question\"][\"identifiant\"][\"numero\"]\n",
    "        # legislature = fichier[\"question\"][\"identifiant\"][\"legislature\"]\n",
    "        date = fichier[\"question\"][\"textesReponse\"][\"texteReponse\"][\"infoJO\"][\"dateJO\"]\n",
    "        texte = fichier[\"question\"][\"textesReponse\"][\"texteReponse\"][\"texte\"]\n",
    "        return {\n",
    "            \"numero\": numero,\n",
    "            # \"legislature\": legislature,\n",
    "            \"date\": date,\n",
    "            \"texte\": texte\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors de l'extraction des informations : {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_info(json.load(open(fichiers[100], \"r\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Application au corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erreur lors de l'extraction des informations : 'NoneType' object is not subscriptable\n"
     ]
    }
   ],
   "source": [
    "# j'applique la fonction sur chaque √©l√©ment du corpus\n",
    "t = [get_info(json.load(open(fichier, \"r\"))) for fichier in fichiers]\n",
    "# je filtre les valeurs nulles\n",
    "t = [i for i in t if i is not None]\n",
    "# je mets sous une forme de dataframe\n",
    "df = pd.DataFrame(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>numero</th>\n",
       "      <th>date</th>\n",
       "      <th>texte</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3063</td>\n",
       "      <td>03/06/2020</td>\n",
       "      <td>&lt;/p&gt;&lt;p align=\"CENTER\"&gt; R√îLE DES COLLECTIVIT√âS ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3599</td>\n",
       "      <td>09/12/2020</td>\n",
       "      <td>&lt;/p&gt;&lt;p align=\"CENTER\"&gt; CONTR√îLE DES EXPORTATIO...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>89</td>\n",
       "      <td>10/08/2017</td>\n",
       "      <td>&lt;/p&gt;&lt;p align=\"CENTER\"&gt; TAXE DE S√âJOUR &lt;a name=...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>348</td>\n",
       "      <td>29/11/2017</td>\n",
       "      <td>&lt;/p&gt;&lt;p align=\"CENTER\"&gt; LISTE NOIRE DES PARADIS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3433</td>\n",
       "      <td>21/10/2020</td>\n",
       "      <td>&lt;/p&gt;&lt;p align=\"CENTER\"&gt; LUTTE CONTRE LE FINANCE...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  numero        date                                              texte\n",
       "0   3063  03/06/2020  </p><p align=\"CENTER\"> R√îLE DES COLLECTIVIT√âS ...\n",
       "1   3599  09/12/2020  </p><p align=\"CENTER\"> CONTR√îLE DES EXPORTATIO...\n",
       "2     89  10/08/2017  </p><p align=\"CENTER\"> TAXE DE S√âJOUR <a name=...\n",
       "3    348  29/11/2017  </p><p align=\"CENTER\"> LISTE NOIRE DES PARADIS...\n",
       "4   3433  21/10/2020  </p><p align=\"CENTER\"> LUTTE CONTRE LE FINANCE..."
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une fois qu'on a les donn√©es, on peut s'inqui√©ter de leur qualit√©\n",
    "\n",
    "-> on veut pas les balises HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nettoyer les donn√©es (pr√©processing)\n",
    "\n",
    "Enlever les balises HTML et les autres soucis : enlever tous les √©l√©ments qui ont la forme \\<XXXXXX>\n",
    "\n",
    "Pour cela on peut utiliser une regex. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remarque sur les regex\n",
    "\n",
    "- biblioth√®que `re` ou `regex`\n",
    "- `re.sub` pour remplacer une partie d'une cha√Æne par une autre\n",
    "- `re.findall` pour trouver toutes les occurrences d'une regex dans une cha√Æne\n",
    "- [Aller voir une cheatsheet ü§ì](https://www.pythoncheatsheet.org/cheatsheet/regular-expressions) ou utiliser un [site d√©di√©](https://regex101.com/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ceci est un test  <cou'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "re.sub(r\"<.*?>\", \"\", \"Ceci est un <b>test</b> <coucou> <cou\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Appliquer au corpus : on d√©finit une fonction de nettoyage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Fonction qui nettoie le texte en supprimant les balises HTML et les espaces inutiles\n",
    "    :param text: le texte √† nettoyer\n",
    "    :return: le texte nettoy√©\n",
    "    \"\"\"\n",
    "    text = re.sub(r\"<.*?>\", \"\", text)\n",
    "    text = re.sub(r\"\\s\\s+\", \" \", text)\n",
    "    # text = text.capitalize()\n",
    "    # autres r√®gles\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On teste la fonction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ceci est un texte. avec un paragraphe coucou'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_text(\"Ceci est un texte.    <p>avec un paragraphe</p> COUCOU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Application au corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>texte</th>\n",
       "      <th>texte_net</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;/p&gt;&lt;p align=\"CENTER\"&gt; R√îLE DES COLLECTIVIT√âS ...</td>\n",
       "      <td>R√îLE DES COLLECTIVIT√âS LOCALES DANS LA LUTTE C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;/p&gt;&lt;p align=\"CENTER\"&gt; CONTR√îLE DES EXPORTATIO...</td>\n",
       "      <td>CONTR√îLE DES EXPORTATIONS D'ARMEMENT M.¬†le pr√©...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;/p&gt;&lt;p align=\"CENTER\"&gt; TAXE DE S√âJOUR &lt;a name=...</td>\n",
       "      <td>TAXE DE S√âJOUR M.¬†le pr√©sident. La parole est ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;/p&gt;&lt;p align=\"CENTER\"&gt; LISTE NOIRE DES PARADIS...</td>\n",
       "      <td>LISTE NOIRE DES PARADIS FISCAUX M.¬†le pr√©siden...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;/p&gt;&lt;p align=\"CENTER\"&gt; LUTTE CONTRE LE FINANCE...</td>\n",
       "      <td>LUTTE CONTRE LE FINANCEMENT DU TERRORISME M.¬†l...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               texte  \\\n",
       "0  </p><p align=\"CENTER\"> R√îLE DES COLLECTIVIT√âS ...   \n",
       "1  </p><p align=\"CENTER\"> CONTR√îLE DES EXPORTATIO...   \n",
       "2  </p><p align=\"CENTER\"> TAXE DE S√âJOUR <a name=...   \n",
       "3  </p><p align=\"CENTER\"> LISTE NOIRE DES PARADIS...   \n",
       "4  </p><p align=\"CENTER\"> LUTTE CONTRE LE FINANCE...   \n",
       "\n",
       "                                           texte_net  \n",
       "0  R√îLE DES COLLECTIVIT√âS LOCALES DANS LA LUTTE C...  \n",
       "1  CONTR√îLE DES EXPORTATIONS D'ARMEMENT M.¬†le pr√©...  \n",
       "2  TAXE DE S√âJOUR M.¬†le pr√©sident. La parole est ...  \n",
       "3  LISTE NOIRE DES PARADIS FISCAUX M.¬†le pr√©siden...  \n",
       "4  LUTTE CONTRE LE FINANCEMENT DU TERRORISME M.¬†l...  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"texte_net\"] = df[\"texte\"].apply(clean_text)\n",
    "df[[\"texte\",\"texte_net\"]].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.loc[1,\"texte\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"./data/dataframe.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ls data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse √† l'√©chelle des mots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chercher la pr√©sence d'un mot\n",
    "\n",
    "Les bases de la fouille de donn√©es. Quels sont les questions qui parlent d'intelligence artificielle ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtre = df[\"texte\"].str.contains(\"coucou\")\n",
    "# df[filtre].loc[2912, \"texte\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chercher un contexte d'un mot avec uen expression r√©guli√®re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"rante ans, <i>Vol au-dessus d'un nid de coucou</i> d√©non√ßait une vision totalitaire de\"]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(\".{40}coucou.{40}\", df[filtre].loc[2912, \"texte\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(df[\"texte\"].str.lower()\n",
    "            .str.contains(\"intelligence artificielle\")\n",
    "            .sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Et si on cherche plusieurs termes ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "termes = [\"intelligence artificielle\", \"IA\", \"algorithme\"]\n",
    "\n",
    "(df[\"texte\"].str.lower()\n",
    "            .str.contains(\"|\".join(termes))\n",
    "            .sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Faire une recherche sur toutes les variables possibles de l'IA\n",
    "\n",
    "- intelligence artificelle\n",
    "- algorithme\n",
    "- AI\n",
    "- ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenisation\n",
    "\n",
    "D√©couper un texte"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utiliser les regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ceci', 'est', 'un', 'test']"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "word_pattern = r\"\\w+\"\n",
    "tokens = re.findall(word_pattern, \"Ceci est un test\")\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [r√¥le, des, collectivit√©s, locales, dans, la, ...\n",
       "1       [contr√¥le, des, exportations, d, armement, m, ...\n",
       "2       [taxe, de, s√©jour, m, le, pr√©sident, la, parol...\n",
       "3       [liste, noire, des, paradis, fiscaux, m, le, p...\n",
       "4       [lutte, contre, le, financement, du, terrorism...\n",
       "                              ...                        \n",
       "4845    [fermetures, de, classes, en, milieu, rural, m...\n",
       "4846    [strat√©gie, en, mati√®re, de, d√©pistage, m, le,...\n",
       "4847    [situation, de, l, entreprise, earta, m, le, p...\n",
       "4848    [limitation, de, la, vitesse, autoris√©e, sur, ...\n",
       "4849    [application, de, la, loi, denormandie, m, le,...\n",
       "Name: texte_net, Length: 4850, dtype: object"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"texte_net\"].apply(lambda x: re.findall(r\"\\w+\",x.lower()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utiliser une premi√®re biblioth√®que : `nltk`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/emilien/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Ceci', 'est', 'un', 'test']"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "word_tokenize(\"Ceci est un test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [R√îLE, DES, COLLECTIVIT√âS, LOCALES, DANS, LA, ...\n",
       "1       [CONTR√îLE, DES, EXPORTATIONS, D'ARMEMENT, M., ...\n",
       "2       [TAXE, DE, S√âJOUR, M., le, pr√©sident, ., La, p...\n",
       "3       [LISTE, NOIRE, DES, PARADIS, FISCAUX, M., le, ...\n",
       "4       [LUTTE, CONTRE, LE, FINANCEMENT, DU, TERRORISM...\n",
       "                              ...                        \n",
       "4845    [FERMETURES, DE, CLASSES, EN, MILIEU, RURAL, M...\n",
       "4846    [STRAT√âGIE, EN, MATI√àRE, DE, D√âPISTAGE, M., le...\n",
       "4847    [SITUATION, DE, L'ENTREPRISE, EARTA, M., le, p...\n",
       "4848    [LIMITATION, DE, LA, VITESSE, AUTORIS√âE, SUR, ...\n",
       "4849    [APPLICATION, DE, LA, LOI, DENORMANDIE, M., le...\n",
       "Name: texte_net, Length: 4850, dtype: object"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"texte_net\"].apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quels sont les termes les plus fr√©quents ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "compteur = Counter([j for i in list(df[\"texte_net\"].apply(word_tokenize)) for j in i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(',', 242083),\n",
       " ('.', 191144),\n",
       " ('de', 187067),\n",
       " ('la', 107423),\n",
       " ('le', 89111),\n",
       " ('et', 87945),\n",
       " ('les', 81402),\n",
       " ('√†', 76522),\n",
       " ('des', 76003),\n",
       " ('du', 48548),\n",
       " ('que', 43008),\n",
       " ('en', 41947),\n",
       " ('sur', 39774),\n",
       " ('pour', 33251),\n",
       " ('est', 31121),\n",
       " ('qui', 30337),\n",
       " ('ministre', 30064),\n",
       " ('nous', 28026),\n",
       " ('!', 25437),\n",
       " ('dans', 24672)]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compteur.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quelles sont les expressions qui reviennent le plus souvent ?\n",
    "\n",
    "Utilisons les bigrammes et les trigrammes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import ngrams\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def generate_bigrams_nltk(text):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    bigrams = list(ngrams(tokens, 2))\n",
    "    return bigrams\n",
    "\n",
    "#generate_bigrams_nltk(df[\"texte_net\"].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ici-m√™me', ',', 'y', \"'\", \"a-t'il\"]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(\"ici-m√™me, y'a-t'il\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Enlever les stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/emilien/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download(\"stopwords\")\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "french_stopwords = list(set(stopwords.words(\"french\")))\n",
    "french_stopwords[0:10]\n",
    "\n",
    "\n",
    "def generate_bigrams_nltk(text):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    filtered_tokens = [token for token in tokens if token.isalnum() and token not in french_stopwords]\n",
    "    bigrams = list(ngrams(filtered_tokens, 2))\n",
    "    return bigrams\n",
    "\n",
    "#generate_bigrams_nltk(df[\"texte_net\"].iloc[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bancs groupe              15134\n",
       "pr√©sident parole           9976\n",
       "applaudissements bancs     9119\n",
       "premier ministre           8117\n",
       "bancs groupes              6324\n",
       "                          ...  \n",
       "fin ann√©e                   282\n",
       "monsieur secr√©taire         281\n",
       "dani√®le obono               278\n",
       "grand d√©bat                 277\n",
       "commission europ√©enne       276\n",
       "Length: 300, dtype: int64"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count bigrams:\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words=french_stopwords, ngram_range=(2, 2), max_features=300)\n",
    "bigrammes = (\n",
    "    pd.DataFrame(\n",
    "        vectorizer.fit_transform(df[\"texte_net\"]).toarray(),\n",
    "        columns=vectorizer.get_feature_names_out(),\n",
    "    )\n",
    "    .T.sum(axis=1)\n",
    "    .sort_values(ascending=False)\n",
    ")\n",
    "bigrammes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Repr√©senter les textes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pr√©sence de mots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dim1</th>\n",
       "      <th>dim2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   dim1  dim2\n",
       "0     0     0\n",
       "1     0     0\n",
       "2     0     0\n",
       "3     0     0\n",
       "4     0     0"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"dim1\"] = df[\"texte_net\"].str.contains(\"intelligence artificielle\")\n",
    "df[\"dim2\"] = df[\"texte_net\"].str.contains(\"science\")\n",
    "df[[\"dim1\",\"dim2\"]].replace({True:1,False:0}).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vecteur brut : Document term matrix / tableau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# cr√©er mon object de ML\n",
    "vectorizer = CountVectorizer(stop_words=french_stopwords, \n",
    "                             ngram_range=(1, 1), \n",
    "                             max_features=800)\n",
    "\n",
    "# appliquer sur les donn√©es\n",
    "X = vectorizer.fit_transform(df[\"texte_net\"])\n",
    "X = pd.DataFrame(X.toarray(),columns=list(vectorizer.get_feature_names_out()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X.iloc[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Une version un peu plus avanc√©e\n",
    "\n",
    "- Term Frequency-Inverse Document Frequency\n",
    "    - Am√©lioration du DTM\n",
    "- Approche souvent utilis√©e pour mettre en valeur les mots les plus sp√©cifiques\n",
    "- `Scikit-learn` a `TfidfVectorizer`\n",
    "\n",
    "$$\\text{TF-IDF}(t, d, D) = \\left( \\frac{f_{t,d}}{n_d} \\right) \\times \\log \\left(\\frac{N}{\\text{df}_t} \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "000           0.000000\n",
       "pauvret√©      0.000000\n",
       "pays          0.000000\n",
       "pense         0.000000\n",
       "permet        0.000000\n",
       "                ...   \n",
       "protection    0.130034\n",
       "relance       0.155877\n",
       "commission    0.213689\n",
       "√©cologique    0.323248\n",
       "transition    0.525928\n",
       "Name: 100, Length: 800, dtype: float64"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# cr√©er un objet\n",
    "vectorizer = TfidfVectorizer(stop_words=french_stopwords, \n",
    "                             ngram_range=(1, 1), \n",
    "                             max_features=800)\n",
    "\n",
    "# applique \n",
    "X = vectorizer.fit_transform(df[\"texte_net\"])\n",
    "\n",
    "# mettre en forme\n",
    "X = pd.DataFrame(X.toarray(),columns=list(vectorizer.get_feature_names_out()))\n",
    "X.loc[100].sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Faire la matrice TF-IDF, identifier les mots qui ont le score le plus important"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distance entre deux textes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.20955574]])"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "X = vectorizer.fit_transform(df[\"texte_net\"])\n",
    "cosine_similarity(X[0], X[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances = pd.DataFrame(pairwise_distances(X, metric=\"cosine\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances[10].idxmax()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application : Faire un nuage de mots avec WordCloud\n",
    "\n",
    "Un coup d'oeil √† la [documentation](https://amueller.github.io/word_cloud/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
